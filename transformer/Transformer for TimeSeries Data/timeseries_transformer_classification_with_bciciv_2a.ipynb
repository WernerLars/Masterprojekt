{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "## Data import\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from keras import utils as np_utils\n",
    "from keras import backend as K"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "folder_path_string = \"BCICIV_2a_gdf/\"\n",
    "\n",
    "file_path_string = folder_path_string + \"A01T.gdf\"\n",
    "evaluation_path_string = folder_path_string + \"A01T.evt\"\n",
    "\n",
    "raws = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\Lars\\Desktop\\transformer\\Transformer for TimeSeries Data\\BCICIV_2a_gdf\\A01T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 672527  =      0.000 ...  2690.108 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lars\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mne\\io\\edf\\edf.py:1123: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "c:\\users\\lars\\appdata\\local\\programs\\python\\python37\\lib\\contextlib.py:119: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 2 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth highpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 8 (effective, after forward-backward)\n",
      "- Cutoff at 2.00 Hz: -6.02 dB\n",
      "\n",
      "Used Annotations descriptions: ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "event_id  [0 0 3]\n",
      "<Epochs |  288 events (all good), 0 - 1 sec, baseline off, ~12.2 MB, data loaded,\n",
      " '7': 72\n",
      " '8': 72\n",
      " '9': 72\n",
      " '10': 72>\n",
      "X.Shape (288, 22, 251)\n",
      "X_train shape: (230, 22, 251, 1)\n",
      "X_test shape: (58, 22, 251, 1)\n",
      "y.Shape (288,)\n",
      "y_train shape: (230, 4)\n",
      "y_test shape: (58, 4)\n",
      "230 train samples\n",
      "58 test samples\n",
      "num_classes: 4\n"
     ]
    }
   ],
   "source": [
    "# while the default tensorflow ordering is 'channels_last' we set it here\n",
    "# to be explicit in case if the user has changed the default ordering\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "##################### Process, filter and epoch the data ######################\n",
    "raw_fname = file_path_string\n",
    "event_fname = evaluation_path_string\n",
    "tmin, tmax = -0., 1\n",
    "\n",
    "# Setup for reading the raw data\n",
    "raw = mne.io.read_raw_gdf(raw_fname)\n",
    "raw.load_data()\n",
    "\n",
    "#original filter\n",
    "raw.filter(2, None, method='iir')  # replace baselining with high-pass\n",
    "\n",
    "events, event_ids = mne.events_from_annotations(raw)\n",
    "event_id = events[1]\n",
    "print(\"event_id \",event_id)\n",
    "picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,\n",
    "                       exclude='bads')\n",
    "picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False)\n",
    "\n",
    "# Read epochs\n",
    "stims =[value for key, value in event_ids.items() if key in ('769','770','771','772')]\n",
    "epochs = mne.Epochs(raw, events, stims, tmin, tmax, proj=False,\n",
    "                    picks=picks, baseline=None, preload=True, verbose=False)\n",
    "channels_to_remove = ['EOG-left', 'EOG-central', 'EOG-right']\n",
    "epochs = epochs.drop_channels(channels_to_remove)\n",
    "labels = epochs.events[:, -1]\n",
    "print(epochs)\n",
    "\n",
    "# extract raw data. scale by 1000 due to scaling sensitivity in deep learning\n",
    "X = epochs.get_data()*1000 # format is in (trials, channels, samples)\n",
    "y = labels\n",
    "y[y == 7] = 0\n",
    "y[y == 8] = 1\n",
    "y[y == 9] = 2\n",
    "y[y == 10] = 3\n",
    "\n",
    "kernels, chans, samples = 1, X.shape[1], X.shape[2]\n",
    "\n",
    "# take 50/25/25 percent of the data to train/validate/test\n",
    "\n",
    "train_idx = round(X.shape[0] * 0.8)\n",
    "#test_idx = round(X.shape[0] * 0.75)\n",
    "\n",
    "X_train      = X[0:train_idx,]\n",
    "Y_train      = y[0:train_idx]\n",
    "#X_validate   = X[train_idx:test_idx,]\n",
    "#Y_validate   = y[train_idx:test_idx]\n",
    "X_test       = X[train_idx:,]\n",
    "Y_test       = y[train_idx:]\n",
    "\n",
    "############################# EEGNet portion ##################################\n",
    "\n",
    "# convert labels to one-hot encodings.\n",
    "y_unique = np.unique(y)\n",
    "num_classes = len(y_unique)\n",
    "Y_train      = np_utils.to_categorical(Y_train-1, num_classes=num_classes)\n",
    "#Y_validate   = np_utils.to_categorical(Y_validate-1, num_classes=num_classes)\n",
    "Y_test       = np_utils.to_categorical(Y_test-1, num_classes=num_classes)\n",
    "\n",
    "# convert data to NHWC (trials, channels, samples, kernels) format. Data\n",
    "# contains 60 channels and 151 time-points. Set the number of kernels to 1.\n",
    "X_train      = X_train.reshape(X_train.shape[0], chans, samples, kernels)\n",
    "#X_validate   = X_validate.reshape(X_validate.shape[0], chans, samples, kernels)\n",
    "X_test       = X_test.reshape(X_test.shape[0], chans, samples, kernels)\n",
    "\n",
    "print(\"X.Shape\", X.shape)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print(\"y.Shape\", y.shape)\n",
    "print('y_train shape:', Y_train.shape)\n",
    "print('y_test shape:', Y_test.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print('num_classes:', num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build the model\n",
    "\n",
    "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
    "where `sequence length` is the number of time steps and `features` is each input\n",
    "timeseries.\n",
    "\n",
    "You can replace your classification RNN layers with this one: the\n",
    "inputs are fully compatible!\n",
    "\n",
    "We include residual connections, layer normalization, and dropout.\n",
    "The resulting layer can be stacked multiple times.\n",
    "\n",
    "The projection layers are implemented through `keras.layers.Conv1D`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = keras.layers.Dropout(dropout)(x)\n",
    "    x = keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main part of our model is now complete. We can stack multiple of those\n",
    "`transformer_encoder` blocks and we can also proceed to add the final\n",
    "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
    "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
    "our model down to a vector of features for each data point in the current\n",
    "batch. A common way to achieve this is to use a pooling layer. For\n",
    "this example, a `GlobalAveragePooling1D` layer is sufficient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling2D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=64,\n",
    "    num_heads=2,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=1,\n",
    "    mlp_units=[8],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 22, 251, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 22, 251, 1)  897         ['input_1[0][0]',                \n",
      " dAttention)                                                      'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 22, 251, 1)   0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 22, 251, 1)  2           ['dropout[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 22, 251, 1)  0           ['layer_normalization[0][0]',    \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 22, 251, 4)   8           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 22, 251, 4)   0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 22, 251, 1)   5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 22, 251, 1)  2           ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 22, 251, 1)  0           ['layer_normalization_1[0][0]',  \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 22)          0           ['tf.__operators__.add_1[0][0]'] \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 8)            184         ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 8)            0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            36          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,134\n",
      "Trainable params: 1,134\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")'''\n",
    "# compile the model and set the optimizers\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics = ['accuracy', 'TruePositives', 'TrueNegatives'])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001BAF4C11F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001BAF4C11F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "92/92 [==============================] - ETA: 0s - loss: 1.3872 - accuracy: 0.2283 - true_positives: 0.0000e+00 - true_negatives: 552.0000WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001BAF89D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001BAF89D05E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "92/92 [==============================] - 486s 5s/step - loss: 1.3872 - accuracy: 0.2283 - true_positives: 0.0000e+00 - true_negatives: 552.0000 - val_loss: 1.3858 - val_accuracy: 0.2826 - val_true_positives: 0.0000e+00 - val_true_negatives: 138.0000\n",
      "Epoch 2/2\n",
      "92/92 [==============================] - 494s 5s/step - loss: 1.3870 - accuracy: 0.2391 - true_positives: 0.0000e+00 - true_negatives: 552.0000 - val_loss: 1.3854 - val_accuracy: 0.2826 - val_true_positives: 0.0000e+00 - val_true_negatives: 138.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=2,\n",
    "    batch_size=2,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "#model.evaluate(X_test, Y_test, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.3885267972946167, 1.3870253562927246], 'accuracy': [0.27826085686683655, 0.25217390060424805], 'true_positives': [0.0, 0.0], 'true_negatives': [345.0, 345.0], 'val_loss': [1.3871972560882568, 1.386379599571228], 'val_accuracy': [0.2068965584039688, 0.2068965584039688], 'val_true_positives': [0.0, 0.0], 'val_true_negatives': [87.0, 87.0]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt  # summarize history for accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.axis([0,1,0,1])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.axis([0,1,0,3])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}